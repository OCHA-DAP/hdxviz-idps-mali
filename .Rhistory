# output
return(out)
}
queryHDX(all = T)
base_url <- "https://test2-data.hdx.rwlabs.org/hdx/public/api2/values"
query_url <- paste(base_url, "?pageNum=1", sep = "")
query_url
doc <- getURL(query_url)
data <- fromJSON(doc)
nRes = data$totalCount
nREs
nRes
nPages = data$totalNumOfPages
nPages
queryHDX <- function(v = 1,
e = NULL,
it = NULL,
minTime = NULL,
maxTime = NULL,
all = F) {
# storing the url versions in a data.frame
versions <- data.frame(url = "https://test2-data.hdx.rwlabs.org/hdx/public/api2/values", id = 1)
# building the url to be queried
base_url <- paste(as.character(versions$url[v]), "?", sep = "")
if (is.null(it) == F) it_url <- paste("it=", it, "", "&", sep = "")
if (is.null(e) == F) e_url <- paste("e=", e, "&", sep = "")
if (is.null(minTime) == F) minTime_url <- paste("e=", minTime, "&", sep = "")
if (is.null(maxTime) == F) maxTime_url <- paste("e=", maxtime, "&", sep = "")
query_url <- paste(base_url,
ifelse(is.null(it) == F, it_url, ""),
ifelse(is.null(e) == F, e_url, ""),
ifelse(is.null(minTime) == F, minTime_url, ""),
ifelse(is.null(maxTime) == F, maxTime_url, ""),
sep = "")
# if all
if (all == T) query_url <- paste(base_url, "pageNum=1", sep = "")
# querying API
doc <- getURL(query_url)
data <- fromJSON(doc)
# error handler
nRes = data$totalCount
if (length(data$results) == 0) stop('The API returned ', nRes, ' results.')
else message('The API returned ', nRes, ' results')
## parsing results
# for all data
if (all == T) {
nPages = data$totalNumOfPages
pb <- txtProgressBar(min = 0, max = length(data$results), style = 3, char = ".")
for (i in 1:nPages) {
setTxtProgressBar(pb, i)
query_url <- paste(base_url, "pageNum=", i, sep = "")
doc <- getURL(query_url)
data <- fromJSON(doc)
if (i == 1) out <- data.frame(data$results[i])
else out <- rbind(out, data.frame(data$results[i]))
}
}
else {
pb <- txtProgressBar(min = 0, max = length(data$results), style = 3, char = ".")
for (i in 1:length(data$results)) {
setTxtProgressBar(pb, i)
if (i == 1) out <- data.frame(data$results[i])
else out <- rbind(out, data.frame(data$results[i]))
}
}
# output
return(out)
}
queryHDX(all = T)
library(rmongodb)
mongo.get.databases()
mg2 <- mongo.create()
mongo.get.databases(mg2)
mongo.get.databases(mg2)
db <- mongo.create()
mongo.get.databases(db)
mongo.get.database.collections(db, 'db')
mongo.get.database.collections(db, 'locations')
mongo.get.database.collections(db, 'indicators')
mongo.get.database.collections(db)
mongo.get.database.collections(db, 'gaza')
mongo.get.databases(db)
mongo.get.database.collections(db, 'gaza')
?rmonbodb
?rmongodb
??rmongodb
mongo.is.connected(db)
if (mongo.is.connected(mongo)) message('Db connection alright.')
else message('Check connection.')
if (mongo.is.connected(db)) message('Db connection alright.')
else message('Check connection.')
# small connection test
if (mongo.is.connected(db)) message('Db connection alright.')
else message('Check connection.')
if (!mongo.is.connected(db)) message('Db connection alright.')
if (!mongo.is.connected(db)) message('Issue: Check connection.')
if (!mongo.is.connected(db)) stop('Issue: Check database connection.')
mongo.get.database.collections(db, 'gaza')
mongo.get.database.collections(db, 'gaza')
mongo.count(db, 'gaza.values')
mongo.count(db, 'gaza.datasets')
mongo.count(db, 'gaza.locatiosn')
mongo.count(db, 'gaza.locations')
mongo.count(db, 'gaza.indicators')
indicators <- mongo.find.all(db, 'gaza.indicators')
View(indicators)
indicators <- mongo.find.all(db, 'gaza.indicators')
getIndicators <- function() {
# connecting to database
db <- mongo.create()  # necessary step
mongo.get.databases(db)
if (mongo.is.connected(db)) {
indicators <- mongo.find.all(db, 'gaza.indicators')
}
}
getIndicators <- function() {
# connecting to database
db <- mongo.create()  # necessary step
mongo.get.databases(db)
if (mongo.is.connected(db)) {
indicators <<- mongo.find.all(db, 'gaza.indicators')
}
}
getIndicators()
View(indicators)
getIndicators <- function() {
# connecting to database
db <- mongo.create()  # necessary step
mongo.get.databases(db)
if (mongo.is.connected(db)) {
indicators <<- mongo.find.all(db, 'gaza.indicators')
message('Use View(indicators) to see the results.')
}
}
getIndicators()
View(indicators)
View(indicators)
indicators <- mongo.find.all(db, 'gaza.ingest')
View(indicators)
# get the data back from a particular table
getCollection <- function(col = NULL) {
# test
if (is.null(col)) stop('provide a collection name')
# connecting to database
db <- mongo.create()  # necessary step
mongo.get.databases(db)
if (mongo.is.connected(db)) {
indicators <<- mongo.find.all(db, paste0('gaza.', col))
message('*****Use View(indicators) to see the results.*****')
}
}
# get the data back from a particular table
getCollection <- function(col = NULL) {
# test
if (is.null(col)) stop('provide a collection name')
# connecting to database
db <- mongo.create()  # necessary step
mongo.get.databases(db)
if (mongo.is.connected(db)) {
indicators <<- mongo.find.all(db, paste0('gaza.', col))
message('*****Use View() to see the results.*****')
}
}
getCollection()
getCollection('indicators')
# get the data back from a particular table
getCollection <- function(col = NULL) {
# test
if (is.null(col)) stop('provide a collection name')
# connecting to database
db <- mongo.create()  # necessary step
mongo.get.databases(db)
if (mongo.is.connected(db)) {
mongo.find.all(db, paste0('gaza.', col))
message('*****Use View() to see the results.*****')
}
}
getCollection()
getCollection('indicator')
getCollection('indicators')
# get the data back from a particular table
getCollection <- function(col = NULL) {
# test
if (is.null(col)) stop('provide a collection name')
# connecting to database
db <- mongo.create()  # necessary step
mongo.get.databases(db)
if (mongo.is.connected(db)) {
x <- mongo.find.all(db, paste0('gaza.', col))
message('*****Use View() to see the results.*****')
}
return(x)
}
getCollection('ingest')
library(XML)
library(RCurl)
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en', sep="")
year = 2014
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en', sep="")
list_url
# getting a list of documents
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en/', sep="")
list_url
table <- getNodeSet(htmlParse(list_url),"//li")[[1]]
table
table <- getNodeSet(htmlParse(list_url),'//*[@id="content"]/div/div[1]/ul/li')[[1]]
table
doc <- readHTMLTable(table, useInternal = TRUE)
doc
class(doc)
doc <- htmlParse(list_url)
doc
link_it <- data.frame(a =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li', xmlGetAttr, 'href'))
link_it
doc <- htmlParse(list_url)
link_it <- data.frame(date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li', xmlGetAttr))
link_it <- data.frame(date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li', xmlGetValue))
?xpathSApply
link_it <- data.frame(date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li', xmlValue))
link_it
View(link_it)
link_it <- data.frame(date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue))
View(link_it)
link_it <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue)
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, 'href')
)
link_it <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, 'href')
)
View(link_it)
# function that gets the list of documents from WHO
# website and assembles a nice data.frame
scrapeList <- function(year = 2014) {
message('Assembling a list of documents.')
# list of urls
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en/', sep="")
# getting the html
doc <- htmlParse(list_url)
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, 'href'),
name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
year = year,
)
return(output)
}
updateList <- scrapeList()
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a[@href]', xmlGetAttr, "href"),
name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
year = year,
)
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a[@href]', xmlGetAttr, "href"),
name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
year = year
)
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"),
name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
year = year
)
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"))
View(output)
x <- data.frame(name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue))
View(x)
nrow(x)
nrow(output)
View(x)
View(output)
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"),
year = year)
View(output)
nrow(output)
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"),
name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue)
year = year)
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"),
name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
year = year)
View(x)
# function that gets the list of documents from WHO
# website and assembles a nice data.frame
scrapeList <- function(year = 2014) {
message('Assembling a list of documents.')
# list of urls
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en/', sep="")
# getting the html
doc <- htmlParse(list_url)
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href")
year = year)
# ISSUE
# apparently the name field is returning an extra name somewhere
# name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
return(output)
}
updateList <- scrapeList()
# function that gets the list of documents from WHO
# website and assembles a nice data.frame
scrapeList <- function(year = 2014) {
message('Assembling a list of documents.')
# list of urls
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en/', sep="")
# getting the html
doc <- htmlParse(list_url)
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"),
year = year)
# ISSUE
# apparently the name field is returning an extra name somewhere
# name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
return(output)
}
updateList <- scrapeList()
View(updateList)
# function that gets the list of documents from WHO
# website and assembles a nice data.frame
scrapeList <- function(year = 2014) {
message('Assembling a list of documents.')
# list of urls
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en/', sep="")
# getting the html
doc <- htmlParse(list_url)
cat('.')
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"),
year = year)
cat('.')
# ISSUE
# apparently the name field is returning an extra name somewhere
# name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
cat('.')
cat(' done')
return(output)
}
updateList <- scrapeList()
# function that gets the list of documents from WHO
# website and assembles a nice data.frame
scrapeList <- function(year = 2014) {
print('Assembling a list of documents.')
# list of urls
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en/', sep="")
# getting the html
doc <- htmlParse(list_url)
cat('.')
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"),
year = year)
cat('.')
# ISSUE
# apparently the name field is returning an extra name somewhere
# name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
cat('.')
cat(' done')
return(output)
}
updateList <- scrapeList()
# function that gets the list of documents from WHO
# website and assembles a nice data.frame
scrapeList <- function(year = 2014) {
cat('Assembling a list of documents.')
# list of urls
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en/', sep="")
# getting the html
doc <- htmlParse(list_url)
cat('.')
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"),
year = year)
cat('.')
# ISSUE
# apparently the name field is returning an extra name somewhere
# name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
cat('.')
cat(' done')
return(output)
}
updateList <- scrapeList()
View(updateList)
View(updateList)
# function that gets the list of documents from WHO
# website and assembles a nice data.frame
scrapeList <- function(year = 2014) {
cat('Assembling a list of documents.')
# list of urls
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en/', sep="")
# getting the html
doc <- htmlParse(list_url)
cat('.')
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"),
year = year)
cat('.')
# ISSUE
# apparently the name field is returning an extra name somewhere
# name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
cat('.')
cat('done.')
# fixing URLs
output$url <- sub('/entity/', 'http://www.who.int/', output$url)
return(output)
}
updateList <- scrapeList()
VieW(updateList)
View(updateList)
# function that gets the list of documents from WHO
# website and assembles a nice data.frame
scrapeList <- function(year = 2014) {
cat('Assembling a list of documents.')
# list of urls
list_url = paste('http://www.who.int/csr/don/archive/year/', year, '/en/', sep="")
# getting the html
doc <- htmlParse(list_url)
cat('.')
# XPath to the PDF link only
output <- data.frame(
date =  xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlValue),
url = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/a', xmlGetAttr, "href"),
year = year)
cat('.')
# ISSUE
# apparently the name field is returning an extra name somewhere
# name = xpathSApply(doc, '//*[@id="content"]/div/div[1]/ul/li/span', xmlValue),
cat('.')
cat('done.')
# fixing URLs
output$url <- sub('/entity/', 'http://www.who.int/', output$url)
output$url <- sub('index.html', '', output$url)
return(output)
}
updateList <- scrapeList()
View(updateList)
View(updateList)
library(RCurl)
library(XML)
letters
class(letters)
for (i in 1:letters) {
print(toupper(letter[i]))
}
let <- as.list(letters)
let
for (i in 1:letters) {
print(toupper(let[i]))
}
for (i in 1:length(letters)) {
print(toupper(let[i]))
}
for (i in 1:length(letters)) {
print(toupper(letters[i]))
}
for (i in 1:length(letters)) {
print(toupper(letters[i]))
}
for (i in 1:length(letters)) {
query_url <- paste0(base_url, toupper(letters[i]))
}
base_url = "http://www.imsdb.com/alphabetical/"
for (i in 1:length(letters)) {
query_url <- paste0(base_url, toupper(letters[i]))
}
for (i in 1:length(letters)) {
query_url <- paste0(base_url, toupper(letters[i]))
print(paste0(base_url, toupper(letters[i])))
}
setwd("~/Documents/Programming/idps_mali_map")
library(reshape2)
library(ggplot2)
data <- read.csv('data/output_2.csv')
View(data)
x <- melt(data)
View(x)
View(data)
x$variable <- sub("X", "", x$variable)
x$variable <- sub("\\.", "-", x$variable)
x$variable <- sub("\\.", "-", x$variable)
x$variable <- as.Date(x$variable)
names(x) <- c('name', 'date', 'value')
View)x
View(x)
y <- dcast(x, date ~ name)
View(y)
